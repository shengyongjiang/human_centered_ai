# OPENMATB STUDY: AUTOMATION BIAS IN HUMAN-IN-THE-LOOP SYSTEMS

**Name:** Sandesh Gavhane, Vaibhav Sharma, Shengyong Jiang  
**Student ID:** 785448, 784575, 781413  
**Course:** Masters in Artificial Intelligence  
**Module:** Human-Centered Trustworthy AI: Human Centered AI - CS5076  
**University:** University zu LÃ¼beck  
**Date:** 09 June February 2025

---

## Terminology

**Automation bias** is the tendency for humans to favor decisions generated by automated systems, like artificial intelligence (AI), even when those systems are flawed or when human judgment would be more accurate. This over-reliance on automation can lead to errors, especially in complex situations where human intuition and experience are crucial.

**Human-in-the-loop (HITL) AI** refers to a system where humans are actively involved in the development, training, and operation of AI systems, providing oversight and corrections to ensure accuracy, reliability, and alignment with human values. Instead of fully autonomous AI, HITL systems integrate human expertise at various stages, such as data labeling, model evaluation, and decision-making, creating a collaborative and adaptable approach.

---

## MOTIVATION

### Problem / Research Gap

Automation bias occurs when humans over-rely on automated systems, even when these systems provide incorrect information or fail. In human-in-the-loop systems, this bias can lead to critical errors where humans fail to detect automation failures or override incorrect automated decisions. Current research lacks understanding of how the frequency of automated alerts specifically influences automation bias in multitasking environments typical of aviation and control room operations.

### Relevance and Interest

This problem is highly relevant as modern aircraft, medical devices, and industrial control systems increasingly rely on automated alerts and warnings. Recent aviation incidents (e.g., Boeing 737 MAX crashes) highlight the dangers of automation bias where pilots failed to appropriately respond to or override automated systems. Understanding how alert frequency affects human reliance behaviour is crucial for designing safer human-machine interfaces that maintain appropriate human oversight.

---

## THEORETICAL BACKGROUND

This study directly addresses key concepts from the lecture:

- **Automation Bias:** The tendency to over-rely on automated information
- **Human-in-the-Loop Systems:** Systems where humans maintain supervisory control over automated processes
- **Trust Calibration:** Matching human trust levels to actual system reliability
- **Cognitive Load Theory:** How task demands affect decision-making quality

### Differentiation from Other Topics

Unlike studies focusing on explanation quality or interface design, this research specifically examines temporal factors (alert frequency) and their impact on automation bias. It differs from pure automation studies by maintaining human decision-making authority while varying the frequency of automated input.

---

## RESEARCH QUESTION

How does the frequency of automated alerts affect automation bias in a multitasking environment?

**Specifically:** Do participants show greater automation bias (over-reliance on automated suggestions) when alerts are frequent compared to when alerts are infrequent, even when alert accuracy remains constant?

---

## HYPOTHESIS

### Primary Hypothesis

Participants receiving frequent automated alerts will demonstrate greater automation bias compared to participants receiving infrequent alerts, as measured by their tendency to follow incorrect automated recommendations.

### Rationale

Frequent alerts may create habituation, leading participants to automatically follow recommendations without critical evaluation. Conversely, infrequent alerts may maintain participants' attention and critical thinking, reducing blind reliance on automation.

### Operational Definition

Automation bias will be measured as the percentage of times participants follow incorrect automated recommendations during system monitoring tasks.

---

## CONCEPT

### Approach

Using OpenMATB's system monitoring task, we will implement an automated alert system that provides recommendations for responding to system anomalies. The key manipulation is alert frequency:

- **High Frequency Group:** Alerts every 30-45 seconds
- **Low Frequency Group:** Alerts every 2-3 minutes

Both groups receive the same overall number of correct and incorrect alerts but distributed differently over time. We measure how often participants follow incorrect recommendations in each condition.

### Theoretical Framework

Based on Signal Detection Theory and Cognitive Resource Theory, frequent alerts may lead to:

- Reduced attention to individual alerts (habituation)
- Cognitive overload reducing critical evaluation
- Increased reliance on automation to manage workload

---

## METHOD

### Study Design

**Between-subjects design** with two conditions:

- **Condition A:** High frequency alerts (every 30-45 seconds)
- **Condition B:** Low frequency alerts (every 2-3 minutes)

**Dependent Variables:**
- Automation bias rate (% of incorrect alerts followed)
- Response time to alerts
- Overall task performance accuracy

### Implementation Plan

#### OpenMATB Customization

**System Monitoring Enhancement:**
- Add automated alert system for gauge anomalies
- Implement recommendation pop-ups ("Recommend: Adjust Tank 2")
- Program 80% accuracy rate for both conditions
- Log all participant responses and timing

**Alert Scheduling:**
- **High Frequency:** 12-16 alerts per 20-minute session
- **Low Frequency:** 6-8 alerts per 20-minute session
- Same proportion of correct/incorrect alerts in both conditions
- Randomize alert timing within frequency constraints

### Consideration for Study

#### Experimental Controls

- **Alert Accuracy:** Both groups receive 80% accurate alerts
- **Task Difficulty:** Identical OpenMATB configuration for both groups
- **Training:** Standardized 10-minute training emphasizing participant autonomy
- **Randomization:** Random assignment to conditions

#### Validity Considerations

- **External Validity:** OpenMATB simulates real aviation monitoring tasks
- **Internal Validity:** Between-subjects design eliminates carryover effects
- **Construct Validity:** Clear operational definition of automation bias

#### Potential Challenges

- **Sample Size:** Need sufficient power to detect medium effect sizes
- **Individual Differences:** Control for prior experience with automated systems
- **Demand Characteristics:** Ensure participants don't realize the study focuses on following/ignoring recommendations

#### Success Metrics

- **Primary:** Significant difference in automation bias rates between conditions
- **Effect Size:** Cohen's d > 0.5 for practical significance
- **Secondary:** Correlation between subjective trust and objective behavior

#### Expected Timeline

- **Week 1:** OpenMATB programming and alert system implementation
- **Week 2:** Pilot testing and refinement
- **Week 3:** Data collection (10 participants)
- **Week 4:** Data analysis and report writing